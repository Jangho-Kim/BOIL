{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from maml.utils import load_dataset, load_model, update_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 episodes test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_task(dataset):\n",
    "    sample_task = dataset.sample_task()\n",
    "    for idx, (image, label) in enumerate(sample_task['train']):\n",
    "        if idx == 0:\n",
    "            s_images = image.unsqueeze(0)\n",
    "            s_labels = [label]\n",
    "            s_real_labels = [sample_task['train'].index[label]]\n",
    "        else:\n",
    "            s_images = torch.cat([s_images, image.unsqueeze(0)], dim=0)\n",
    "            s_labels.append(label)\n",
    "            s_real_labels.append(sample_task['train'].index[label])\n",
    "    \n",
    "    for idx, (image, label) in enumerate(sample_task['test']):\n",
    "        if idx == 0:\n",
    "            q_images = image.unsqueeze(0)\n",
    "            q_labels = [label]\n",
    "            q_real_labels = [sample_task['test'].index[label]]\n",
    "        else:\n",
    "            q_images = torch.cat([q_images, image.unsqueeze(0)], dim=0)\n",
    "            q_labels.append(label)\n",
    "            q_real_labels.append(sample_task['test'].index[label])\n",
    "    \n",
    "    s_labels = torch.tensor(s_labels).type(torch.LongTensor)\n",
    "    s_real_labels = torch.tensor(s_real_labels).type(torch.LongTensor)\n",
    "    q_labels = torch.tensor(q_labels).type(torch.LongTensor)\n",
    "    q_real_labels = torch.tensor(q_real_labels).type(torch.LongTensor)\n",
    "    return [s_images, s_labels, s_real_labels, q_images, q_labels, q_real_labels]\n",
    "\n",
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def get_arguments(path, dataset, save_name):\n",
    "    filename = '{}/{}_{}/logs/arguments.txt'.format(path, dataset, save_name)\n",
    "\n",
    "    args = easydict.EasyDict()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            key, val = line.split(\": \")\n",
    "            if '\\n' in val:\n",
    "                val = val[:-1]\n",
    "            if isfloat(val):\n",
    "                if val.isdigit():\n",
    "                    val = int(val)\n",
    "                else:\n",
    "                    val = float(val)\n",
    "            if val == 'True' or val == 'False':\n",
    "                val = val == 'True'\n",
    "            args[key] = val\n",
    "    return args\n",
    "\n",
    "def print_accuracy(args, test_dataset, sample_tasks, iteration, inner_update_num, NIL_testing=False):\n",
    "    device = torch.device(args.device)\n",
    "    sample_number = len(sample_tasks)\n",
    "    \n",
    "    index = ['task{}'.format(str(i+1)) for i in range(sample_number)]\n",
    "    columns = []\n",
    "    columns += ['Accuracy on support set (before adaptation)', 'Accuracy on query set (before adaptation)']\n",
    "    for i in range(inner_update_num):\n",
    "        columns += ['Accuracy on support set (after {} adaptation(s))'.format(i+1),\n",
    "                    'Accuracy on query set (after {} adaptation(s))'.format(i+1)]\n",
    "    \n",
    "    if NIL_testing:\n",
    "        filename_pd = '{}/{}/logs/{}_nil_results_{}.csv'.format(args.output_folder, args.save_dir, test_dataset, iteration)\n",
    "    else:\n",
    "        filename_pd = '{}/{}/logs/{}_results_{}.csv'.format(args.output_folder, args.save_dir, test_dataset, iteration)\n",
    "    test_pd = pd.DataFrame(np.zeros([sample_number, len(columns)]), index=index, columns=columns)\n",
    "    \n",
    "    model = load_model(args)\n",
    "    checkpoint = '{}/{}/models/best_val_acc_model.pt'.format(args.output_folder, args.save_dir)    \n",
    "    checkpoint = torch.load(checkpoint, map_location=device)\n",
    "    \n",
    "    step_size = OrderedDict()\n",
    "    \n",
    "    if args.model == '4conv_sep':\n",
    "        for name, _ in model.named_parameters():\n",
    "            if 'classifier' in name:\n",
    "                step_size[name] = args.classifier_step_size\n",
    "            else:\n",
    "                if 'conv'+args.save_name[-1] in name:\n",
    "                    step_size[name] = args.extractor_step_size\n",
    "                else:\n",
    "                    step_size[name] = 0.0\n",
    "    else:\n",
    "        for name, _ in model.named_parameters():\n",
    "            if 'classifier' in name:\n",
    "                step_size[name] = args.classifier_step_size\n",
    "            else:\n",
    "                step_size[name] = args.extractor_step_size\n",
    "    \n",
    "    for idx in tqdm(range(sample_number)):\n",
    "        task_log = []\n",
    "               \n",
    "        model.load_state_dict(checkpoint, strict=True)\n",
    "        model.to(device)\n",
    "\n",
    "        support_input = sample_tasks[idx][0].to(device)\n",
    "        support_target = sample_tasks[idx][1].to(device)\n",
    "        support_real_target = sample_tasks[idx][2]\n",
    "        query_input = sample_tasks[idx][3].to(device)\n",
    "        query_target = sample_tasks[idx][4].to(device)\n",
    "        query_real_target = sample_tasks[idx][5]\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # before adaptation\n",
    "        support_features, support_logit = model(support_input)\n",
    "        _, support_pred_target = torch.max(support_logit, dim=1)\n",
    "                \n",
    "        query_features, query_logit = model(query_input)\n",
    "        _, query_pred_target = torch.max(query_logit, dim=1)\n",
    "        \n",
    "        if NIL_testing:\n",
    "            cos = nn.CosineSimilarity()\n",
    "            support_features_mean = torch.zeros([args.num_ways, support_features.shape[1]]).to(device)\n",
    "            support_target_mean = torch.zeros([args.num_ways]).to(device)\n",
    "            for label in range(args.num_ways):\n",
    "                support_features_mean[label] = torch.mean(support_features[torch.where(support_target==label)], dim=0)\n",
    "                support_target_mean[label] = label\n",
    "\n",
    "            distance = torch.zeros([len(query_features), len(support_features_mean)])\n",
    "            for i, query_feature in enumerate(query_features):\n",
    "                distance[i] = cos(torch.cat([query_feature.unsqueeze(0)]*len(support_features_mean)), support_features_mean)\n",
    "            top_similar_idx = torch.argmax(distance, dim=1)\n",
    "            \n",
    "            query_pred_target = support_target_mean[top_similar_idx]\n",
    "        \n",
    "        task_log.append((sum(support_target==support_pred_target)/float(len(support_target))).item())\n",
    "        task_log.append((sum(query_target==query_pred_target)/float(len(query_target))).item())\n",
    "        \n",
    "        # after adaptation\n",
    "        params = None\n",
    "        for _ in range(inner_update_num):            \n",
    "            support_features, support_logit = model(support_input, params=params)\n",
    "            inner_loss = F.cross_entropy(support_logit, support_target)\n",
    "\n",
    "            model.zero_grad()\n",
    "            params = update_parameters(model=model,\n",
    "                                       loss=inner_loss,\n",
    "                                       params=params,\n",
    "                                       step_size=step_size,\n",
    "                                       first_order=args.first_order)\n",
    "            model.load_state_dict(params, strict=True)\n",
    "\n",
    "            support_features, support_logit = model(support_input)\n",
    "            _, support_pred_target = torch.max(support_logit, dim=1)  \n",
    "\n",
    "            query_features, query_logit = model(query_input)\n",
    "            _, query_pred_target = torch.max(query_logit, dim=1)\n",
    "        \n",
    "            if NIL_testing:            \n",
    "                cos = nn.CosineSimilarity()\n",
    "                support_features_mean = torch.zeros([args.num_ways, support_features.shape[1]]).to(device)\n",
    "                support_target_mean = torch.zeros([args.num_ways]).to(device)\n",
    "                for label in range(args.num_ways):\n",
    "                    support_features_mean[label] = torch.mean(support_features[torch.where(support_target==label)], dim=0)\n",
    "                    support_target_mean[label] = label\n",
    "\n",
    "                distance = torch.zeros([len(query_features), len(support_features_mean)])\n",
    "                for i, query_feature in enumerate(query_features):\n",
    "                    distance[i] = cos(torch.cat([query_feature.unsqueeze(0)]*len(support_features_mean)), support_features_mean)\n",
    "                top_similar_idx = torch.argmax(distance, dim=1)\n",
    "\n",
    "                query_pred_target = support_target_mean[top_similar_idx]\n",
    "            \n",
    "            task_log.append((sum(support_target==support_pred_target)/float(len(support_target))).item())\n",
    "            task_log.append((sum(query_target==query_pred_target)/float(len(query_target))).item())\n",
    "\n",
    "        test_pd.iloc[idx] = task_log\n",
    "    test_pd.loc[sample_number+1], test_pd.loc[sample_number+2] = test_pd.mean(axis=0), test_pd.std(axis=0)\n",
    "    test_pd.index = list(test_pd.index[:sample_number]) + ['mean', 'std']\n",
    "    test_pd.to_csv(filename_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 1000\n",
    "inner_update_num = 1\n",
    "dataset = [# ('miniimagenet', 'miniimagenet'),\n",
    "           # ('miniimagenet', 'tieredimagenet'),\n",
    "           # ('miniimagenet', 'cars'),\n",
    "           ('cars', 'cars'),\n",
    "           # ('cars', 'miniimagenet'),\n",
    "           # ('cars', 'cub')\n",
    "          ]\n",
    "\n",
    "model = '4conv'\n",
    "path = './output_head_abal'\n",
    "\n",
    "# model = '4conv'\n",
    "# path = './output_head_abal'\n",
    "\n",
    "for train_dataset, test_dataset in dataset:\n",
    "    for iteration in [1, 2, 3, 4, 5]:\n",
    "        for num_shots in [5]: # 1,5\n",
    "            dataset_args = easydict.EasyDict({'folder': '/home/osilab7/hdd/ml_dataset/',\n",
    "                                              'dataset': test_dataset,\n",
    "                                              'num_ways': 5,\n",
    "                                              'num_shots': num_shots,\n",
    "                                              'download': True})\n",
    "\n",
    "            sample_tasks = [make_sample_task(load_dataset(dataset_args, 'meta_test')) for _ in tqdm(range(sample_number))]\n",
    "                        \n",
    "            for algorithm in ['MAML_s', 'MAML_m', 'MAML_ll']:\n",
    "                save_name = '{}shot_{}_{}'.format(num_shots, model, algorithm)\n",
    "                args = get_arguments(path, train_dataset, save_name)\n",
    "                args.device = 'cuda:0'\n",
    "                \n",
    "                print_accuracy(args, test_dataset, sample_tasks, iteration=iteration, inner_update_num=inner_update_num, NIL_testing=False)\n",
    "                # print_accuracy(args, test_dataset, sample_tasks, iteration=iteration, inner_update_num=inner_update_num, NIL_testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [('miniimagenet', 'cars')]\n",
    "\n",
    "model = '4conv'\n",
    "path = './output'\n",
    "\n",
    "for train_dataset, test_dataset in dataset:\n",
    "    for num_shots in [1, 5]:\n",
    "        print ('train: {}, test: {}, shot: {}'.format(train_dataset, test_dataset, num_shots))\n",
    "        for algorithm in ['MAML', 'ANIL', 'BOIL']:\n",
    "            save_name = '{}shot_{}_{}'.format(num_shots, model, algorithm)\n",
    "            args = get_arguments(path, train_dataset, save_name)\n",
    "            \n",
    "            log_folder = '{}/{}/logs'.format(args.output_folder, args.save_dir)\n",
    "            test_files = [d for d in os.listdir(log_folder) if test_dataset in d]\n",
    "#             test_files = sorted([d for d in test_files if 'nil' not in d])\n",
    "            test_files = sorted([d for d in test_files if 'nil' in d])\n",
    "    \n",
    "            test_results = []\n",
    "            for f in test_files:\n",
    "                test_results.append(list(pd.read_csv(os.path.join(log_folder, f)).iloc[-2])[-1])\n",
    "            test_results = 100*np.array(test_results)\n",
    "            print ('alg: {}, {:.2f} ± {:.2f}'.format(algorithm,\n",
    "                                                     np.mean(test_results),\n",
    "                                                     np.std(test_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature space, logit space (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_logits(args, sample_task, pos):\n",
    "    device = torch.device(args.device)\n",
    "    \n",
    "    model = load_model(args)\n",
    "    checkpoint = '{}/{}/models/best_val_acc_model.pt'.format(args.output_folder, args.save_dir)    \n",
    "    checkpoint = torch.load(checkpoint, map_location=device)\n",
    "    \n",
    "    step_size = OrderedDict()\n",
    "    \n",
    "    if args.model == '4conv_sep':\n",
    "        for name, _ in model.named_parameters():\n",
    "            if 'classifier' in name:\n",
    "                step_size[name] = args.classifier_step_size\n",
    "            else:\n",
    "                if 'conv'+args.save_name[-1] in name:\n",
    "                    step_size[name] = args.extractor_step_size\n",
    "                else:\n",
    "                    step_size[name] = 0.0\n",
    "    else:\n",
    "        for name, _ in model.named_parameters():\n",
    "            if 'classifier' in name:\n",
    "                step_size[name] = args.classifier_step_size\n",
    "            else:\n",
    "                step_size[name] = args.extractor_step_size\n",
    "    \n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    support_input = sample_task[0].to(device)\n",
    "    support_target = sample_task[1].to(device)\n",
    "    support_real_target = sample_task[2]\n",
    "    query_input = sample_task[3].to(device)\n",
    "    query_target = sample_task[4].to(device)\n",
    "    query_real_target = sample_task[5]\n",
    "\n",
    "    # before adaptation\n",
    "    before_support_features, before_support_logits = model(support_input)\n",
    "    if args.model == '4conv':\n",
    "        if pos == 1:\n",
    "            before_query_features = model.features[0](query_input)\n",
    "        elif pos == 2:\n",
    "            before_query_features = model.features[1](model.features[0](query_input))\n",
    "        elif pos == 3:\n",
    "            before_query_features = model.features[2](model.features[1](model.features[0](query_input)))\n",
    "        elif pos == 4:\n",
    "            before_query_features = model.features[3](model.features[2](model.features[1](model.features[0](query_input))))\n",
    "        elif pos == 5:\n",
    "            before_query_features = model.classifier(model.features[3](model.features[2](model.features[1](model.features[0](query_input)))).view(75, -1))\n",
    "    elif args.model == '4conv_sep':\n",
    "        if pos == 1:\n",
    "            before_query_features = model.conv1(query_input)\n",
    "        elif pos == 2:\n",
    "            before_query_features = model.conv2(model.conv1(query_input))\n",
    "        elif pos == 3:\n",
    "            before_query_features = model.conv3(model.conv2(model.conv1(query_input)))\n",
    "        elif pos == 4:\n",
    "            before_query_features = model.conv4(model.conv3(model.conv2(model.conv1(query_input))))\n",
    "        elif pos == 5:\n",
    "            before_query_features = model.classifier(model.conv4(model.conv3(model.conv2(model.conv1(query_input)))).view(75, -1))\n",
    "#     elif args.model == 'resnet':\n",
    "#         if pos == 1:\n",
    "#             before_query_features = model.layer1(query_input)\n",
    "#         elif pos == 2:\n",
    "#             before_query_features = model.layer2(model.layer1(query_input))\n",
    "#         elif pos == 3:\n",
    "#             before_query_features = model.layer3(model.layer2(model.layer1(query_input)))\n",
    "#         elif pos == 4:\n",
    "#             before_query_features = model.layer4(model.layer3(model.layer2(model.layer1(query_input))))\n",
    "#         elif pos == 5:\n",
    "#             before_query_features = model.classifier(F.avg_pool2d(model.layer4(model.layer3(model.layer2(model.layer1(query_input)))), 5).view(75, -1))\n",
    "#     elif args.model == 'resnet':\n",
    "#         before_block4 = model.layer3(model.layer2(model.layer1(query_input)))\n",
    "#         if pos == 1:\n",
    "#             before_query_features = model.layer4[0].conv1(before_block4)\n",
    "#         elif pos == 2:\n",
    "#             before_query_features = model.layer4[0].bn1(model.layer4[0].conv1(before_block4))\n",
    "#         elif pos == 3:\n",
    "#             before_query_features = model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))\n",
    "#         elif pos == 4:\n",
    "#             before_query_features = model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))  \n",
    "#         elif pos == 5:\n",
    "#             before_query_features = model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))\n",
    "#         elif pos == 6:\n",
    "#             before_query_features = model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))\n",
    "#         elif pos == 7:\n",
    "#             before_query_features = model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))\n",
    "#         elif pos == 8:\n",
    "#             before_query_features = model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))))\n",
    "#         elif pos == 9:\n",
    "#             before_query_features = model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))) + \\\n",
    "#                                    model.layer4[0].downsample(before_block4)\n",
    "#         elif pos == 10:\n",
    "#             before_query_features = model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))) + \\\n",
    "#                                                          model.layer4[0].downsample(before_block4))\n",
    "#         elif pos == 11:\n",
    "#             before_query_features = model.layer4[0].maxpool(model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))) + \\\n",
    "#                                                            model.layer4[0].downsample(before_block4)))\n",
    "\n",
    "    elif args.model == 'resnet': # _b style\n",
    "        before_block4 = model.layer3(model.layer2(model.layer1(query_input)))\n",
    "        if pos == 1:\n",
    "            before_query_features = model.layer4[0].conv1(before_block4)\n",
    "        elif pos == 2:\n",
    "            before_query_features = model.layer4[0].bn1(model.layer4[0].conv1(before_block4))\n",
    "        elif pos == 3:\n",
    "            before_query_features = model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))\n",
    "        elif pos == 4:\n",
    "            before_query_features = model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))  \n",
    "        elif pos == 5:\n",
    "            before_query_features = model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))\n",
    "        elif pos == 6:\n",
    "            before_query_features = model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))\n",
    "        elif pos == 7:\n",
    "            before_query_features = model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))\n",
    "        elif pos == 8:\n",
    "            before_query_features = model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))))\n",
    "        elif pos == 9:\n",
    "            before_query_features = model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))))\n",
    "        elif pos == 10:\n",
    "            before_query_features = model.layer4[0].maxpool(model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))))))\n",
    "    \n",
    "    \n",
    "    before_query_features = before_query_features.view(75, -1)\n",
    "    \n",
    "    # after adaptation\n",
    "    inner_loss = F.cross_entropy(before_support_logits, support_target)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    params = update_parameters(model=model,\n",
    "                               loss=inner_loss,\n",
    "                               params=None,\n",
    "                               step_size=step_size,\n",
    "                               first_order=args.first_order)\n",
    "    model.load_state_dict(params, strict=True)\n",
    "    \n",
    "    if args.model == '4conv':\n",
    "        if pos == 1:\n",
    "            after_query_features = model.features[0](query_input)\n",
    "        elif pos == 2:\n",
    "            after_query_features = model.features[1](model.features[0](query_input))\n",
    "        elif pos == 3:\n",
    "            after_query_features = model.features[2](model.features[1](model.features[0](query_input)))\n",
    "        elif pos == 4:\n",
    "            after_query_features = model.features[3](model.features[2](model.features[1](model.features[0](query_input))))\n",
    "        elif pos == 5:\n",
    "            after_query_features = model.classifier(model.features[3](model.features[2](model.features[1](model.features[0](query_input)))).view(75, -1))\n",
    "    elif args.model == '4conv_sep':\n",
    "        if pos == 1:\n",
    "            after_query_features = model.conv1(query_input)\n",
    "        elif pos == 2:\n",
    "            after_query_features = model.conv2(model.conv1(query_input))\n",
    "        elif pos == 3:\n",
    "            after_query_features = model.conv3(model.conv2(model.conv1(query_input)))\n",
    "        elif pos == 4:\n",
    "            after_query_features = model.conv4(model.conv3(model.conv2(model.conv1(query_input))))\n",
    "        elif pos == 5:\n",
    "            after_query_features = model.classifier(model.conv4(model.conv3(model.conv2(model.conv1(query_input)))).view(75, -1))\n",
    "#     elif args.model == 'resnet':\n",
    "#         if pos == 1:\n",
    "#             after_query_features = model.layer1(query_input)\n",
    "#         elif pos == 2:\n",
    "#             after_query_features = model.layer2(model.layer1(query_input))\n",
    "#         elif pos == 3:\n",
    "#             after_query_features = model.layer3(model.layer2(model.layer1(query_input)))\n",
    "#         elif pos == 4:\n",
    "#             after_query_features = model.layer4(model.layer3(model.layer2(model.layer1(query_input))))\n",
    "#         elif pos == 5:\n",
    "#             after_query_features = model.classifier(F.avg_pool2d(model.layer4(model.layer3(model.layer2(model.layer1(query_input)))), 5).view(75, -1))\n",
    "#     elif args.model == 'resnet':\n",
    "#         before_block4 = model.layer3(model.layer2(model.layer1(query_input)))\n",
    "#         if pos == 1:\n",
    "#             after_query_features = model.layer4[0].conv1(before_block4)\n",
    "#         elif pos == 2:\n",
    "#             after_query_features = model.layer4[0].bn1(model.layer4[0].conv1(before_block4))\n",
    "#         elif pos == 3:\n",
    "#             after_query_features = model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))\n",
    "#         elif pos == 4:\n",
    "#             after_query_features = model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))  \n",
    "#         elif pos == 5:\n",
    "#             after_query_features = model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))\n",
    "#         elif pos == 6:\n",
    "#             after_query_features = model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))\n",
    "#         elif pos == 7:\n",
    "#             after_query_features = model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))\n",
    "#         elif pos == 8:\n",
    "#             after_query_features = model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))))\n",
    "#         elif pos == 9:\n",
    "#             after_query_features = model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))) + \\\n",
    "#                                    model.layer4[0].downsample(before_block4)\n",
    "#         elif pos == 10:\n",
    "#             after_query_features = model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))) + \\\n",
    "#                                                          model.layer4[0].downsample(before_block4))\n",
    "#         elif pos == 11:\n",
    "#             after_query_features = model.layer4[0].maxpool(model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))) + \\\n",
    "#                                                            model.layer4[0].downsample(before_block4)))\n",
    "            \n",
    "    elif args.model == 'resnet': # _b style\n",
    "        before_block4 = model.layer3(model.layer2(model.layer1(query_input)))\n",
    "        if pos == 1:\n",
    "            after_query_features = model.layer4[0].conv1(before_block4)\n",
    "        elif pos == 2:\n",
    "            after_query_features = model.layer4[0].bn1(model.layer4[0].conv1(before_block4))\n",
    "        elif pos == 3:\n",
    "            after_query_features = model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))\n",
    "        elif pos == 4:\n",
    "            after_query_features = model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))  \n",
    "        elif pos == 5:\n",
    "            after_query_features = model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))\n",
    "        elif pos == 6:\n",
    "            after_query_features = model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))\n",
    "        elif pos == 7:\n",
    "            after_query_features = model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))\n",
    "        elif pos == 8:\n",
    "            after_query_features = model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))))\n",
    "        elif pos == 9:\n",
    "            after_query_features = model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4)))))))))\n",
    "        elif pos == 10:\n",
    "            after_query_features = model.layer4[0].maxpool(model.layer4[0].relu3(model.layer4[0].bn3(model.layer4[0].conv3(model.layer4[0].relu2(model.layer4[0].bn2(model.layer4[0].conv2(model.layer4[0].relu1(model.layer4[0].bn1(model.layer4[0].conv1(before_block4))))))))))\n",
    "            \n",
    "    \n",
    "    after_query_features = after_query_features.view(75, -1)\n",
    "    \n",
    "    return (before_query_features.unsqueeze(0).detach().cpu(), after_query_features.unsqueeze(0).detach().cpu())\n",
    "\n",
    "def get_similarity(outputs):\n",
    "    distance = torch.zeros([len(outputs), len(outputs)])\n",
    "    cos = nn.CosineSimilarity()\n",
    "    for i in range(len(outputs)):\n",
    "        distance[i] = cos(outputs, outputs[i].unsqueeze(0))\n",
    "    return distance\n",
    "\n",
    "def get_mean(similarity_matrices):\n",
    "    num_images = 15\n",
    "    \n",
    "    similarity_matrices[range(5*num_images), range(5*num_images)] = 0\n",
    "    same_class_distance = torch.zeros([5*num_images, 5*num_images])\n",
    "    same_class_distance[0*num_images:1*num_images, 0*num_images:1*num_images] = similarity_matrices[0*num_images:1*num_images, 0*num_images:1*num_images]\n",
    "    same_class_distance[1*num_images:2*num_images, 1*num_images:2*num_images] = similarity_matrices[1*num_images:2*num_images, 1*num_images:2*num_images]\n",
    "    same_class_distance[2*num_images:3*num_images, 2*num_images:3*num_images] = similarity_matrices[2*num_images:3*num_images, 2*num_images:3*num_images]\n",
    "    same_class_distance[3*num_images:4*num_images, 3*num_images:4*num_images] = similarity_matrices[3*num_images:4*num_images, 3*num_images:4*num_images]\n",
    "    same_class_distance[4*num_images:5*num_images, 4*num_images:5*num_images] = similarity_matrices[4*num_images:5*num_images, 4*num_images:5*num_images]\n",
    "\n",
    "    different_class_distance = similarity_matrices - same_class_distance\n",
    "\n",
    "    same_class_distance_list = same_class_distance[same_class_distance.nonzero()[:,0], same_class_distance.nonzero()[:,1]]\n",
    "    different_class_distance_list = different_class_distance[different_class_distance.nonzero()[:,0], different_class_distance.nonzero()[:,1]]\n",
    "                \n",
    "    return (torch.mean(same_class_distance_list).item(),\n",
    "            torch.std(same_class_distance_list).item(),\n",
    "            torch.mean(different_class_distance_list).item(),\n",
    "            torch.std(different_class_distance_list).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'miniimagenet'\n",
    "num_shots = 5\n",
    "dataset_args = easydict.EasyDict({'folder': '/home/osilab7/hdd/ml_dataset/',\n",
    "                                  'dataset': dataset,\n",
    "                                  'num_ways': 5,\n",
    "                                  'num_shots': num_shots,\n",
    "                                  'download': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = '4conv'\n",
    "# path = './output'\n",
    "# algorithms = ['MAML', 'ANIL', 'BOIL']\n",
    "model = 'resnet'\n",
    "path = './output_resnet'\n",
    "algorithms = ['MAML_a', 'MAML_b', 'ANIL_a', 'ANIL_b', 'BOIL_a', 'BOIL_b']\n",
    "# algorithms = ['MAML_a', 'ANIL_a', 'BOIL_a']\n",
    "algorithms = ['MAML_b', 'ANIL_b', 'BOIL_b']\n",
    "\n",
    "sample_task = make_sample_task(load_dataset(dataset_args, 'meta_train'))\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    save_name = '{}shot_{}_{}'.format(num_shots, model, algorithm)\n",
    "    args = get_arguments(path, dataset, save_name)\n",
    "    args.device = 'cuda:0'\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, sharey=True, figsize=(20, 3)) # (10, 3), (8, 3)\n",
    "    \n",
    "    axes[0].set_title('Before adaptation')\n",
    "    axes[0].set_ylim([0.0-0.05, 1.0+0.05])\n",
    "    axes[0].tick_params(axis='both', which='major')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].set_title('After adaptation')\n",
    "    axes[1].set_ylim([0.0-0.05, 1.0+0.05])\n",
    "    axes[1].tick_params(axis='both', which='major')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    if model == '4conv':\n",
    "        xrange = ['conv1', 'conv2', 'conv3', 'conv4']\n",
    "        pos_list = [1,2,3,4]\n",
    "#     elif model == 'resnet':\n",
    "#         xrange = ['block1', 'block2', 'block3', 'block4']\n",
    "#         pos_list = [1,2,3,4]\n",
    "#     elif model == 'resnet':\n",
    "#         xrange = ['conv1', 'bn1', 'relu1', 'conv2', 'bn2', 'relu2', 'conv3', 'bn3', 'residual', 'relu3', 'maxpool']\n",
    "#         pos_list = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "    elif model == 'resnet':\n",
    "        xrange = ['conv1', 'bn1', 'relu1', 'conv2', 'bn2', 'relu2', 'conv3', 'bn3', 'relu3', 'maxpool']\n",
    "        pos_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "    \n",
    "    before_same_list = []\n",
    "    before_same_std_list = []\n",
    "    before_different_list = []\n",
    "    before_different_std_list = []\n",
    "    \n",
    "    after_same_list = []\n",
    "    after_same_std_list = []\n",
    "    after_different_list = []\n",
    "    after_different_std_list = []\n",
    "\n",
    "    for pos in pos_list:    \n",
    "        before_f, after_f = get_features_logits(args, sample_task, pos=pos)\n",
    "                \n",
    "        before = get_similarity(outputs=before_f.squeeze())\n",
    "        after = get_similarity(outputs=after_f.squeeze())\n",
    "                \n",
    "        before_same_class, before_same_class_std, before_different_class, before_different_class_std= get_mean(before)\n",
    "        after_same_class, after_same_class_std, after_different_class, after_different_class_std = get_mean(after)\n",
    "\n",
    "        before_same_list.append(before_same_class)\n",
    "        before_same_std_list.append(before_same_class_std)\n",
    "        before_different_list.append(before_different_class)\n",
    "        before_different_std_list.append(before_different_class_std)\n",
    "        \n",
    "        after_same_list.append(after_same_class)\n",
    "        after_same_std_list.append(after_same_class_std)\n",
    "        after_different_list.append(after_different_class)\n",
    "        after_different_std_list.append(after_different_class_std)\n",
    "    \n",
    "    axes[0].errorbar(xrange, before_different_list, yerr=before_different_std_list, fmt='-o', capsize=6, capthick=2)\n",
    "    axes[0].errorbar(xrange, before_same_list, yerr=before_same_std_list, fmt='-o', capsize=6, capthick=2)\n",
    "    \n",
    "    axes[1].errorbar(xrange, after_different_list, yerr=after_different_std_list, fmt='-o', capsize=6, capthick=2)\n",
    "    axes[1].errorbar(xrange, after_same_list, yerr=after_same_std_list, fmt='-o', capsize=6, capthick=2)\n",
    "    \n",
    "#     plt.show()\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "    plt.savefig('./src/{}_cosine_in_block4_miniimagenet.pdf'.format(algorithm), bbox_inches='tight', format='pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient norm when adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = ['conv1_w', 'conv2_w', 'conv3_w', 'conv4_w', 'head_w']\n",
    "maml_values1 = np.array([0.1327, 0.0942, 0.1181, 0.1028, 3.3497])\n",
    "maml_values2 = np.array([0.1706, 0.1169, 0.1470, 0.1172, 3.2524])\n",
    "maml_values3 = np.array([0.1356, 0.1059, 0.1390, 0.1231, 3.2256])\n",
    "maml_values4 = np.array([0.1641, 0.1065, 0.1270, 0.1115, 3.1296])\n",
    "maml_values5 = np.array([0.1375, 0.1045, 0.1297, 0.1140, 3.4852])\n",
    "maml_values = np.mean([maml_values1, maml_values2, maml_values3, maml_values4, maml_values5], axis=0)\n",
    "maml_values_std = np.std([maml_values1, maml_values2, maml_values3, maml_values4, maml_values5], axis=0)\n",
    "\n",
    "anil_values1 = np.array([0., 0., 0., 0., 3.4998])\n",
    "anil_values2 = np.array([0., 0., 0., 0., 3.4090])\n",
    "anil_values3 = np.array([0., 0., 0., 0., 3.3367])\n",
    "anil_values4 = np.array([0., 0., 0., 0., 3.1945])\n",
    "anil_values5 = np.array([0., 0., 0., 0., 3.5396])\n",
    "anil_values = np.mean([anil_values1, anil_values2, anil_values3, anil_values4, anil_values5], axis=0)\n",
    "anil_values_std = np.std([anil_values1, anil_values2, anil_values3, anil_values4, anil_values5], axis=0)\n",
    "\n",
    "boil_values1 = np.array([0.3532, 0.3685, 0.5297, 4.4506, 0.])\n",
    "boil_values2 = np.array([0.5029, 0.4703, 0.6391, 5.0119, 0.])\n",
    "boil_values3 = np.array([0.3648, 0.3765, 0.5192, 4.4990, 0.])\n",
    "boil_values4 = np.array([0.3271, 0.3796, 0.5237, 4.2251, 0.])\n",
    "boil_values5 = np.array([0.4050, 0.4030, 0.5288, 4.3817, 0.])\n",
    "boil_values = np.mean([boil_values1, boil_values2, boil_values3, boil_values4, boil_values5], axis=0)\n",
    "boil_values_std = np.std([boil_values1, boil_values2, boil_values3, boil_values4, boil_values5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.errorbar(xrange, maml_values, yerr=maml_values_std, fmt='-o', capsize=6, capthick=2, label='MAML', color='blue')\n",
    "plt.errorbar(xrange, anil_values, yerr=anil_values_std, fmt='-o', capsize=6, capthick=2, label='ANIL', color='green')\n",
    "plt.errorbar(xrange, boil_values, yerr=boil_values_std, fmt='-o', capsize=6, capthick=2, label='BOIL', color='red')\n",
    "\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('./src/grad_norm.pdf'.format(model), bbox_inches='tight', format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA/CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_linear(x):\n",
    "    \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "    Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "    \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "    Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "      bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "      possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "    Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    dot_products = x.dot(x.T)\n",
    "    sq_norms = np.diag(dot_products)\n",
    "    sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "    sq_median_distance = np.median(sq_distances)\n",
    "    return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "    \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "    This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "    induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "    Args:\n",
    "    gram: A num_examples x num_examples symmetric matrix.\n",
    "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "      estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "    A symmetric matrix with centered columns and rows.\n",
    "    \"\"\"\n",
    "    if not np.allclose(gram, gram.T):\n",
    "        raise ValueError('Input must be a symmetric matrix.')\n",
    "    gram = gram.copy()\n",
    "\n",
    "    if unbiased:\n",
    "        # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "        # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "        # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "        # stable than the alternative from Song et al. (2007).\n",
    "        n = gram.shape[0]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "        means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "        means -= np.sum(means) / (2 * (n - 1))\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "    else:\n",
    "        means = np.mean(gram, 0, dtype=np.float64)\n",
    "        means -= np.mean(means) / 2\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "\n",
    "    return gram\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "    \"\"\"Compute CKA.\n",
    "\n",
    "    Args:\n",
    "    gram_x: A num_examples x num_examples Gram matrix.\n",
    "    gram_y: A num_examples x num_examples Gram matrix.\n",
    "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "    Returns:\n",
    "    The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "    gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "    # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "    # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "    scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "    normalization_x = np.linalg.norm(gram_x)\n",
    "    normalization_y = np.linalg.norm(gram_y)\n",
    "    return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y, n):\n",
    "    \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "    # This formula can be derived by manipulating the unbiased estimator from\n",
    "    # Song et al. (2007).\n",
    "    return (\n",
    "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "    \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "    This is typically faster than computing the Gram matrix when there are fewer\n",
    "    features than examples.\n",
    "\n",
    "    Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "      biased. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "    The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "    features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "    dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "    normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "    normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "    if debiased:\n",
    "        n = features_x.shape[0]\n",
    "        # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "        sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "        sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "        squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "        squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "        dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "            dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "            squared_norm_x, squared_norm_y, n)\n",
    "        normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "            squared_norm_x, squared_norm_x, n))\n",
    "        normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "            squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "    return dot_product_similarity / (normalization_x * normalization_y)\n",
    "\n",
    "def cca(features_x, features_y):\n",
    "    \"\"\"Compute the mean squared CCA correlation (R^2_{CCA}).\n",
    "\n",
    "    Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "    The mean squared CCA correlations between X and Y.\n",
    "    \"\"\"\n",
    "    qx, _ = np.linalg.qr(features_x)  # Or use SVD with full_matrices=False.\n",
    "    qy, _ = np.linalg.qr(features_y)\n",
    "    return np.linalg.norm(qx.T.dot(qy)) ** 2 / min(\n",
    "      features_x.shape[1], features_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cars'\n",
    "num_shots = 5\n",
    "dataset_args = easydict.EasyDict({'folder': '/home/osilab7/hdd/ml_dataset/',\n",
    "                                  'dataset': dataset,\n",
    "                                  'num_ways': 5,\n",
    "                                  'num_shots': num_shots,\n",
    "                                  'download': True})\n",
    "\n",
    "model = '4conv'\n",
    "path = './output'\n",
    "algorithms = ['MAML', 'ANIL', 'BOIL']\n",
    "\n",
    "model = '4conv_sep'\n",
    "path = './output_conv_abal'\n",
    "algorithms = ['MAML_3', 'BOIL_3']\n",
    "\n",
    "\n",
    "\n",
    "maml_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "# anil_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "boil_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "\n",
    "# model = 'resnet'\n",
    "# path = './output_resnet'\n",
    "# algorithms = ['MAML_a', 'MAML_b', 'ANIL_a', 'ANIL_b', 'BOIL_a', 'BOIL_b']\n",
    "\n",
    "# maml_a_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "# maml_b_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "# anil_a_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "# anil_b_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "# boil_a_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "# boil_b_df = pd.DataFrame(columns=['1','2','3','4','head'])\n",
    "\n",
    "for i in tqdm(range(1)):\n",
    "    sample_task = make_sample_task(load_dataset(dataset_args, 'meta_train'))\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        save_name = '{}shot_{}_{}'.format(num_shots, model, algorithm)\n",
    "        args = get_arguments(path, dataset, save_name)\n",
    "        args.device = 'cuda:0'\n",
    "\n",
    "        all_before_f = torch.tensor([])\n",
    "        all_after_f = torch.tensor([])\n",
    "\n",
    "        cka_list = []\n",
    "        cka_std_list = []\n",
    "\n",
    "        if model == '4conv' or model == '4conv_sep':\n",
    "            xrange = ['conv1', 'conv2', 'conv3', 'conv4', 'head']\n",
    "            pos_list = [1,2,3,4,5]\n",
    "        elif model == 'resnet':\n",
    "            xrange = ['block1', 'block2', 'block3', 'block4', 'head']\n",
    "            pos_list = [1,2,3,4,5]\n",
    "\n",
    "        for pos in pos_list:\n",
    "            before_f, after_f = get_features_logits(args, sample_task, pos=pos)\n",
    "\n",
    "            before_f = before_f.squeeze(0).numpy()\n",
    "            after_f = after_f.squeeze(0).numpy()\n",
    "\n",
    "            cka = feature_space_linear_cka(before_f, after_f)\n",
    "            cka_list.append(cka)\n",
    "        \n",
    "        if model == '4conv':\n",
    "            if algorithm == 'MAML':\n",
    "                maml_df.loc[i] = cka_list\n",
    "            elif algorithm ==  'ANIL':\n",
    "                anil_df.loc[i] = cka_list\n",
    "            elif algorithm ==  'BOIL':\n",
    "                boil_df.loc[i] = cka_list\n",
    "        elif model == '4conv_sep':\n",
    "            if 'MAML' in algorithm:\n",
    "                maml_df.loc[i] = cka_list\n",
    "            elif 'BOIL' in algorithm:\n",
    "                boil_df.loc[i] = cka_list\n",
    "        elif model == 'resnet':\n",
    "            if algorithm == 'MAML_a':\n",
    "                maml_a_df.loc[i] = cka_list\n",
    "            if algorithm == 'MAML_b':\n",
    "                maml_b_df.loc[i] = cka_list\n",
    "            if algorithm == 'ANIL_a':\n",
    "                anil_a_df.loc[i] = cka_list\n",
    "            if algorithm == 'ANIL_b':\n",
    "                anil_b_df.loc[i] = cka_list\n",
    "            if algorithm == 'BOIL_a':\n",
    "                boil_a_df.loc[i] = cka_list\n",
    "            if algorithm == 'BOIL_b':\n",
    "                boil_b_df.loc[i] = cka_list\n",
    "                \n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "if model == '4conv':\n",
    "    if algorithm == 'MAML':\n",
    "        ax.plot(xrange, cka_list, marker='o', label='MAML', color='#4F81BD')\n",
    "        ax.errorbar(xrange, cka_list, yerr=cka_std_list, fmt='-o', capsize=6, capthick=2, label='MAML')\n",
    "    elif algorithm == 'BOIL':\n",
    "        ax.plot(xrange, cka_list, marker='D', label='BOIL', color='#C0504D')\n",
    "elif model == 'resnet':\n",
    "    if algorithm == 'block_a_extractor':\n",
    "        ax.plot(xrange, cka_list, marker='o', label='BOIL w/ last skip connection', color='#4F81BD')\n",
    "    elif algorithm == 'block_b_extractor':\n",
    "        ax.plot(xrange, cka_list, marker='D', label='BOIL w/o last skip connection', color='#C0504D')\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "# plt.savefig('./src/{}_cka.pdf'.format(model), bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "\"\"\"\n",
    "\n",
    "maml_df.to_csv('maml_3_df.csv')\n",
    "# anil_df.to_csv('anil_df.csv')\n",
    "boil_df.to_csv('boil_3_df.csv')\n",
    "\n",
    "# maml_a_df.to_csv('maml_a_df.csv')\n",
    "# maml_b_df.to_csv('maml_b_df.csv')\n",
    "# anil_a_df.to_csv('anil_a_df.csv')\n",
    "# anil_b_df.to_csv('anil_b_df.csv')\n",
    "# boil_a_df.to_csv('boil_a_df.csv')\n",
    "# boil_b_df.to_csv('boil_b_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_acc = [np.mean(maml_df['1']), np.mean(maml_df['2']), np.mean(maml_df['3']), np.mean(maml_df['4']), np.mean(maml_df['head'])]\n",
    "maml_std = [np.std(maml_df['1']), np.std(maml_df['2']), np.std(maml_df['3']), np.std(maml_df['4']), np.std(maml_df['head'])]\n",
    "# anil_acc = [np.mean(anil_df['1']), np.mean(anil_df['2']), np.mean(anil_df['3']), np.mean(anil_df['4']), np.mean(anil_df['head'])]\n",
    "# anil_std = [np.std(anil_df['1']), np.std(anil_df['2']), np.std(anil_df['3']), np.std(anil_df['4']), np.std(anil_df['head'])]\n",
    "boil_acc = [np.mean(boil_df['1']), np.mean(boil_df['2']), np.mean(boil_df['3']), np.mean(boil_df['4']), np.mean(boil_df['head'])]\n",
    "boil_std = [np.std(boil_df['1']), np.std(boil_df['2']), np.std(boil_df['3']), np.std(boil_df['4']), np.std(boil_df['head'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "xrange = ['conv1', 'conv2', 'conv3', 'conv4', 'head']\n",
    "\n",
    "ax.set_title('CKA')\n",
    "ax.set_ylim([0.0-0.05, 1.0+0.05])\n",
    "ax.tick_params(axis='both', which='major')\n",
    "ax.grid(True)\n",
    "\n",
    "ax.errorbar(xrange, maml_acc, yerr=maml_std, fmt='-o', capsize=6, capthick=2, label='MAML', color='blue')\n",
    "# ax.errorbar(xrange, anil_acc, yerr=anil_std, fmt='-o', capsize=6, capthick=2, label='ANIL', color='green')\n",
    "ax.errorbar(xrange, boil_acc, yerr=boil_std, fmt='-o', capsize=6, capthick=2, label='BOIL', color='red')\n",
    "        \n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "# plt.savefig('./src/{}_pos3_cka.pdf'.format(model), bbox_inches='tight', format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_a_acc = [np.mean(maml_a_df['1']), np.mean(maml_a_df['2']), np.mean(maml_a_df['3']), np.mean(maml_a_df['4']), np.mean(maml_a_df['head'])]\n",
    "maml_a_std = [np.std(maml_a_df['1']), np.std(maml_a_df['2']), np.std(maml_a_df['3']), np.std(maml_a_df['4']), np.std(maml_a_df['head'])]\n",
    "maml_b_acc = [np.mean(maml_b_df['1']), np.mean(maml_b_df['2']), np.mean(maml_b_df['3']), np.mean(maml_b_df['4']), np.mean(maml_b_df['head'])]\n",
    "maml_b_std = [np.std(maml_b_df['1']), np.std(maml_b_df['2']), np.std(maml_b_df['3']), np.std(maml_b_df['4']), np.std(maml_b_df['head'])]\n",
    "anil_a_acc = [np.mean(anil_a_df['1']), np.mean(anil_a_df['2']), np.mean(anil_a_df['3']), np.mean(anil_a_df['4']), np.mean(anil_a_df['head'])]\n",
    "anil_a_std = [np.std(anil_a_df['1']), np.std(anil_a_df['2']), np.std(anil_a_df['3']), np.std(anil_a_df['4']), np.std(anil_a_df['head'])]\n",
    "anil_b_acc = [np.mean(anil_b_df['1']), np.mean(anil_b_df['2']), np.mean(anil_b_df['3']), np.mean(anil_b_df['4']), np.mean(anil_b_df['head'])]\n",
    "anil_b_std = [np.std(anil_b_df['1']), np.std(anil_b_df['2']), np.std(anil_b_df['3']), np.std(anil_b_df['4']), np.std(anil_b_df['head'])]\n",
    "boil_a_acc = [np.mean(boil_a_df['1']), np.mean(boil_a_df['2']), np.mean(boil_a_df['3']), np.mean(boil_a_df['4']), np.mean(boil_a_df['head'])]\n",
    "boil_a_std = [np.std(boil_a_df['1']), np.std(boil_a_df['2']), np.std(boil_a_df['3']), np.std(boil_a_df['4']), np.std(boil_a_df['head'])]\n",
    "boil_b_acc = [np.mean(boil_b_df['1']), np.mean(boil_b_df['2']), np.mean(boil_b_df['3']), np.mean(boil_b_df['4']), np.mean(boil_b_df['head'])]\n",
    "boil_b_std = [np.std(boil_b_df['1']), np.std(boil_b_df['2']), np.std(boil_b_df['3']), np.std(boil_b_df['4']), np.std(boil_b_df['head'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "xrange = ['block1', 'block2', 'block3', 'block4', 'head']\n",
    "\n",
    "ax.set_title('CKA')\n",
    "ax.set_ylim([0.0-0.05, 1.0+0.05])\n",
    "ax.tick_params(axis='both', which='major')\n",
    "ax.grid(True)\n",
    "\n",
    "ax.errorbar(xrange, maml_a_acc, yerr=maml_a_std, fmt='-o', capsize=6, capthick=2, label='MAML w/ LSC', color='blue')\n",
    "ax.errorbar(xrange, anil_a_acc, yerr=anil_a_std, fmt='-o', capsize=6, capthick=2, label='ANIL w/ LSC', color='green')\n",
    "ax.errorbar(xrange, boil_a_acc, yerr=boil_a_std, fmt='-o', capsize=6, capthick=2, label='BOIL w/ LSC', color='red')\n",
    "        \n",
    "ax.errorbar(xrange, maml_b_acc, yerr=maml_b_std, fmt='--x', capsize=6, capthick=2, label='MAML w/o LSC', color='blue')\n",
    "ax.errorbar(xrange, anil_b_acc, yerr=anil_b_std, fmt='--x', capsize=6, capthick=2, label='ANIL w/o LSC', color='green')\n",
    "ax.errorbar(xrange, boil_b_acc, yerr=boil_b_std, fmt='--x', capsize=6, capthick=2, label='BOIL w/o LSC', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "# plt.savefig('./src/{}_cka.pdf'.format(model), bbox_inches='tight', format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
